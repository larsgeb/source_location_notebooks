{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: rgba(220,220,220,0.1) ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
    "    <div style=\"float: left ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.9) ; width: 75% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Hamiltonian Monte Carlo - Notebook 1</div>\n",
    "            <div style=\"font-size: x-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Munich Earth Skience School 2020</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.5)\">Bayesian inference on Earthquake hypocenters accelerated with Hamiltonian Monte Carlo</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Authors:\n",
    "* Lars Gebraad ([@larsgeb](https://github.com/larsgeb))\n",
    "\n",
    "##### Authors of the SeismoLive original:\n",
    "* Heiner Igel ([@heinerigel](https://github.com/heinerigel))\n",
    "\n",
    "* Kilian Ge√üele ([@KGessele](https://github.com/KGessele))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be investigating the applicability of **Hamiltonian Monte Carlo** (HMC) to solving non-linear equations and Bayesian inference on their resulting inverse problems. We will look specificly at the Earthquake source-location problem, as presented also on the [SeismoLive website](https://krischer.github.io/seismo_live_build/html/Seismic%20Inverse%20Problems/Earthquake%20Location/el_hypocenter_wrapper.html). \n",
    "\n",
    "In the first section, we'll define the forward and the inverse problem, but this is not the focus of this notebook. These are merely needed to illustrate HMC. \n",
    "\n",
    "We'll look at the following sections:\n",
    "1. Defining the inverse problem;\n",
    "2. An conceptual comparison of Metropolis-Hastings and Hamiltonian Monte Carlo;\n",
    "3. A more rigorous comparison of Metropolis-Hastings and Hamiltonian Monte Carlo;\n",
    "4. A short interpretation of the physical results;\n",
    "5. Influence of the mass matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.322438Z",
     "start_time": "2020-02-13T16:57:58.043681Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is a configuration step for the exercise\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# These are custom made imports\n",
    "import samplers\n",
    "from misc import marginal_grid\n",
    "\n",
    "# Some style settings\n",
    "plt.style.use('ggplot')\n",
    "font = {'size'   : 18}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to work on a different inverse problem (e.g. other test functions), create a class that is structured as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.326404Z",
     "start_time": "2020-02-13T16:57:58.324001Z"
    }
   },
   "outputs": [],
   "source": [
    "class target_example:\n",
    "    def misfit(self, m):\n",
    "        return misfit\n",
    "\n",
    "    def grad(self, m):\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setting up the inverse problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short preparatory section, we will set up a basic hypocenter location inverse problem with unknown medium velocity. We will do this in three steps: \n",
    "\n",
    "1. First, we create a forward model predicting arrival times of earthquakes given a hypocenter location and bulk medium velocity. \n",
    "2. Then we create 'observed' data and together with observational uncertainties and prior information, compile everything into a target object, or posterior distribution. \n",
    "3. Lastly, because we want to perform HMC sampling, we also will define the gradient of the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of the Forward Problem\n",
    "For any arbitrary model, we can solve the forward problem using the following equation.\n",
    "\n",
    "$$\n",
    "\\mathbf{d} = g(\\mathbf{m})\n",
    "$$\n",
    "\n",
    "where the function $g$ describes the physical processes that associate the model $\\mathbf{m}$ with the theoretical arrival times $\\mathbf{d}$. In this exercise, we use a very simple geometrical concept and assume that the medium is homogeneous and the wave velocity $v$ is constant. Therefore, the theoretical arrival time $t_i$ at one particular station location {$x_i, z_i$} for an arbitrary source {$x, z, T$} in a medium with bulk velocity {$v$} is given by the following equation:\n",
    "\n",
    "\n",
    "$$\n",
    "t_i = g_i(x, z, T, v) = T + \\frac{\\sqrt{(x - x_i)^2 + (x - z_i)^2}}{v}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.340336Z",
     "start_time": "2020-02-13T16:57:58.327705Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Defining the forward problem\n",
    "def forward(m, station_x, station_z):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    # Dissassemble the model vector\n",
    "    x = m[0]\n",
    "    z = m[1]\n",
    "    T = m[2]\n",
    "    v = m[3]\n",
    "\n",
    "    # Create an empty column vector for the data\n",
    "    t_calc = np.empty((station_x.size, 1), dtype=np.float64)\n",
    "\n",
    "    # Loop over the stations\n",
    "    for istat in range(station_x.size):\n",
    "\n",
    "        # Implement the formula for Earthquake first arrival times -------------\n",
    "\n",
    "        t_calc[istat] = None # < here\n",
    "        \n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        # Solution\n",
    "        t_calc[istat] = (\n",
    "            T\n",
    "            + (1.0 / v)\n",
    "            * ((x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0) ** 0.5\n",
    "        )\n",
    "\n",
    "    return t_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure of fit\n",
    "\n",
    "To perform an inversions (or inference), we need some criterion that assigns values to each model. This is typically done by comparing the simulated data (synthetics) to the observed data, and mapping these to a scalar function. This function, the misfit, is a measure of how 'good' a model is with respect to the observed data.\n",
    "\n",
    "A typical choice would be the 'L2 misfit'. For the Earthquake source location this is is a quadratic function in residual traveltime (synthetic minus observed). It is given as:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{m}, \\mathbf{t}_{obs}) = \n",
    "k\\exp\\Big(-\\frac{1}{2} \\sum_{i}\\big(\\frac{t_i(\\mathbf{m}) - t_i^{obs}}{\\sigma_i}\\big)^2\\Big)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}, \\mathbf{t}_{obs}) = \n",
    "\\frac{1}{2} \\sum_{i}\\left(\\frac{t_i(\\mathbf{m}) - t_i^{obs}}{\\sigma_i}\\right)^2\n",
    "$$  \n",
    "\n",
    "Here we define the Earthquake inversion measure of fit, or misfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.355974Z",
     "start_time": "2020-02-13T16:57:58.341784Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Defining the misfit of the traveltimes\n",
    "def misfit_tt(\n",
    "    m,\n",
    "    t_obs, # Observations\n",
    "    uncertainties,\n",
    "    station_x, \n",
    "    station_z, \n",
    "    depth_limit, # Prior\n",
    "    v_mean, # Prior\n",
    "    v_std, # Prior\n",
    "):\n",
    "\n",
    "    # Create synthetic data\n",
    "    t_syn = forward(m, station_x, station_z)\n",
    "\n",
    "    # Initialize misfit\n",
    "    misfit = 0.0\n",
    "\n",
    "    # == Likelihood ==\n",
    "    \n",
    "    # Loop over stations\n",
    "    for istat in range(np.size(station_x)):\n",
    "        misfit += ((t_syn[istat] - t_obs[istat]) ** 2) / (\n",
    "            2.0 * uncertainties[istat] * uncertainties[istat]\n",
    "        )\n",
    "\n",
    "    # == Prior == \n",
    "    \n",
    "    z = m[1]\n",
    "    v = m[3]\n",
    "    \n",
    "    # Velocity prior\n",
    "    misfit += ((v - v_mean) ** 2) / (2.0 * v_std * v_std)\n",
    "\n",
    "    # Depth prior\n",
    "    if z < 0.0 or z > depth_limit:\n",
    "        misfit += np.inf\n",
    "\n",
    "    return misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of the data and misfit w.r.t. model parameters\n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}, \\mathbf{t}_{obs}) = \n",
    "\\frac{1}{2} \\sum_{i}\\left(\\frac{t_i(\\mathbf{m}) - t_i^{obs}}{\\sigma_i}\\right)^2\n",
    "$$  \n",
    "\n",
    "More explicitly, we have a scalar valued function that depends on the synthetic data, which in turn depends on the model parameters.\n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}) = f\\left(\\mathbf{d}(\\mathbf{m})\\right)\n",
    "$$  \n",
    "\n",
    "Applying the chain rule for the derivative:\n",
    "\n",
    "$$\n",
    "\\partial_\\mathbf{m} \\chi(\\mathbf{m}) = \\left[\\partial_\\mathbf{d} f\\left(\\mathbf{d}(\\mathbf{m})\\right)\\right] \\cdot \\partial_\\mathbf{m}\\mathbf{d}(\\mathbf{m})\n",
    "$$  \n",
    "\n",
    "With nd datapoints and nm model parameters:\n",
    "\n",
    "$$\n",
    "\\partial_\\mathbf{d} f\\left(\\mathbf{d}(\\mathbf{m})\\right) = 1 \\times nd\\\\\n",
    "\\partial_\\mathbf{m}\\mathbf{d}(\\mathbf{m}) = nd \\times nm\n",
    "$$ \n",
    "\n",
    "making the total gradient:\n",
    "\n",
    "$$\n",
    "\\partial_\\mathbf{m} \\chi(\\mathbf{m}) = 1 \\times nm\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.369282Z",
     "start_time": "2020-02-13T16:57:58.357282Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the gradient of the misfit (and prior) using the chain rule\n",
    "def gradient_data(m, station_x, station_z):\n",
    "    \"\"\"\n",
    "    Returns tensor containing the covariant derivative of the data w.r.t. the model\n",
    "    parameters, i.e. the gradient of a vector function.\n",
    "    \n",
    "    Each column is a parameter, each row a station.\n",
    "    \"\"\"\n",
    "\n",
    "    x = m[0]\n",
    "    z = m[1]\n",
    "    T = m[2]\n",
    "    v = m[3]\n",
    "\n",
    "    grad_t_calc = np.empty((station_x.size, 4), dtype=np.float64)\n",
    "\n",
    "    for istat in range(np.size(station_x)):  # Loop over number of stations\n",
    "\n",
    "        # dd / dx\n",
    "        grad_t_calc[istat, 0] = (1.0 / v) * (\n",
    "            (x - station_x[istat])\n",
    "            / np.sqrt((x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0)\n",
    "        )\n",
    "\n",
    "        # dd / dz\n",
    "        grad_t_calc[istat, 1] = (1.0 / v) * (\n",
    "            (z - station_z[istat])\n",
    "            / np.sqrt((x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0)\n",
    "        )\n",
    "\n",
    "        # dd / dT\n",
    "        grad_t_calc[istat, 2] = 1\n",
    "\n",
    "        # dd / dv\n",
    "        grad_t_calc[istat, 3] = -(1.0 / (v * v)) * np.sqrt(\n",
    "            (x - station_x[istat]) ** 2.0 + (z - station_z[istat]) ** 2.0\n",
    "        )\n",
    "\n",
    "    return grad_t_calc\n",
    "\n",
    "def gradient_misfit_chain_rule(\n",
    "    m,\n",
    "    t_obs,\n",
    "    uncertainties,\n",
    "    station_x,\n",
    "    station_z,\n",
    "    v_mean,\n",
    "    v_std,\n",
    "    depth_limit,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns covector containing the gradient of the misfit w.r.t. the model\n",
    "    parameters, i.e. the gradient of a scalar function. Implemented using\n",
    "    component wise analytical formulas and the chain rule. \n",
    "    \n",
    "    Each column is a parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    t_syn = forward(m, station_x, station_z)\n",
    "    grad_t_syn = dd_dm(m, station_x, station_z)\n",
    "\n",
    "    grad_misfit = np.empty((1, station_x.size))\n",
    "\n",
    "    for istat in range(np.size(station_x)):\n",
    "\n",
    "        grad_misfit[0, istat] = (\n",
    "            2.0\n",
    "            * (t_syn[istat] - t_obs[istat])\n",
    "            / (2.0 * uncertainties[istat] * uncertainties[istat])\n",
    "        )\n",
    "\n",
    "    grad_misfit = grad_misfit @ grad_t_syn\n",
    "\n",
    "    # Applying prior on velocity\n",
    "    v = m[3]\n",
    "    grad_misfit[0, 3] += 2.0 * (v - v_mean) / (2.0 * v_std * v_std)\n",
    "\n",
    "    return grad_misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also consider the entire misfit function only a function of model parameters. That just means that the analytical derivatives will be a bit harder. Many times, this is actually beneficial for implementation speed, because implementations can optimize combined floating point operations. An example implementation is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.384937Z",
     "start_time": "2020-02-13T16:57:58.370516Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the gradient of the misfit (and prior) in one go; analytically\n",
    "def gradient_misfit_analytical(\n",
    "    m, t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns covector containing the gradient of the misfit w.r.t. the model\n",
    "    parameters, i.e. the gradient of a scalar function. Implemented using\n",
    "    analytical derivative.\n",
    "    \n",
    "    Each column is a parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    grad_misfit = np.zeros((1, 4))\n",
    "\n",
    "    x = m[0]\n",
    "    z = m[1]\n",
    "    T = m[2]\n",
    "    v = m[3]\n",
    "\n",
    "    for istat in range(np.size(station_x)):\n",
    "\n",
    "        xs = station_x[istat]\n",
    "        zs = station_z[istat]\n",
    "        t0 = t_obs[istat]\n",
    "\n",
    "        d = ((x - xs) ** 2 + (z - zs) ** 2) ** 0.5\n",
    "\n",
    "        A = 2 * uncertainties[istat] * uncertainties[istat]\n",
    "\n",
    "        grad_misfit[0, 0] += 2 * (x - xs) * (T - t0 + d / v) / (v * d * A)\n",
    "        grad_misfit[0, 1] += 2 * (z - zs) * (T - t0 + d / v) / (v * d * A)\n",
    "        grad_misfit[0, 2] += 2 * (T - t0 + d / v) / A\n",
    "        grad_misfit[0, 3] += -2 * d * (T - t0 + d / v) / (v * v * A)\n",
    "\n",
    "    grad_misfit[0, 3] += 2 * (v - v_mean) / (2 * v_std * v_std)\n",
    "\n",
    "    return grad_misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling everything into something neat (a class)\n",
    "\n",
    "To make working with the data a little easier, we wrap all the settings associated with the inverse problem into a neat package; a class. This 'target' class only needs:\n",
    "* A constructor to set up;\n",
    "* A target.misfit(m) to evaluate a misfit;\n",
    "* A target.grad(m) to evaluate a gradient.\n",
    "\n",
    "We can just re-use the functions for misfits and gradients we defined in the cells before to create this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.400209Z",
     "start_time": "2020-02-13T16:57:58.386053Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Create the class (a sort of template) which contains all the inverse problem components\n",
    "class target_tt:\n",
    "    def __init__(self, t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit):\n",
    "        \n",
    "        # Sanity check\n",
    "        if np.array([station_x, station_z, uncertainties]).size != station_x.size*3:\n",
    "                print('ERROR: \"station_x, station_z, uncertainties\" must have same length')\n",
    "                raise NotImplementedError\n",
    "        \n",
    "        # Assign defaults\n",
    "        self.t_obs = t_obs\n",
    "        self.uncertainties = uncertainties\n",
    "        self.station_x = station_x\n",
    "        self.station_z = station_z\n",
    "        self.v_mean = v_mean\n",
    "        self.v_std = v_std\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "        # Some householding\n",
    "        self.labels = [\n",
    "            \"Horizontal source location\",\n",
    "            \"Vertical source location\",\n",
    "            \"Origin time\",\n",
    "            \"Medium velocity\",\n",
    "        ]\n",
    "        self.units = [\"km\", \"km\", \"s\", \"km/s\"]\n",
    "\n",
    "    # Optional, could also be programmed directly into misfit\n",
    "    def forward(self, m):\n",
    "        return forward(m, self.station_x, self.station_x)\n",
    "        \n",
    "    def misfit(self, m):\n",
    "        return misfit_tt(\n",
    "            m,\n",
    "            t_obs = self.t_obs,\n",
    "            uncertainties=self.uncertainties,\n",
    "            station_x=self.station_x,\n",
    "            station_z=self.station_z,\n",
    "            v_mean=self.v_mean,\n",
    "            v_std=self.v_std,\n",
    "            depth_limit=self.depth_limit,\n",
    "        )\n",
    "\n",
    "    def grad(self, m):\n",
    "        return gradient_misfit_analytical(\n",
    "            m,\n",
    "            t_obs = self.t_obs,\n",
    "            uncertainties=self.uncertainties,\n",
    "            station_x=self.station_x,\n",
    "            station_z=self.station_z,\n",
    "            v_mean=self.v_mean,\n",
    "            v_std=self.v_std,\n",
    "            depth_limit=self.depth_limit,\n",
    "        ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating some 'real' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.567972Z",
     "start_time": "2020-02-13T16:57:58.402769Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define the true event properties and receiver network, and plot these\n",
    "\n",
    "# Define earthquake source properties\n",
    "source_x = 16.0\n",
    "source_z = 15.0\n",
    "origin_T = 17.0\n",
    "v_exact = 5.0\n",
    "\n",
    "# Define station coordinates of the array\n",
    "station_x = np.array([0, 15.0, 30.0])\n",
    "station_z = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6, 6))\n",
    "axes.scatter(station_x, station_z, color=\"b\", marker=\"v\", s=200, label=\"Receivers\")\n",
    "axes.scatter(source_x, source_z, color=\"r\", marker=\".\", s=400, label=\"True location\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlabel(\"Horizontal location [km]\")\n",
    "axes.set_ylabel(\"Vertical location [km]\")\n",
    "axes.plot([-10, 35], [0, 0], \"k\")\n",
    "axes.set_xlim([-10, 35])\n",
    "axes.set_ylim([25, -2])\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.574939Z",
     "start_time": "2020-02-13T16:57:58.569697Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create the 'true' (observed) data and obervational uncertainties\n",
    "\n",
    "# Create a vector out of the true values\n",
    "m_true = np.array([source_x, source_z, origin_T, v_exact])\n",
    "\n",
    "# Calculate observed (exact) arrival times for all stations\n",
    "t_obs = forward(m_true, station_x, station_z)\n",
    "\n",
    "# Define uncertainties for the observed arrival time at each station. These are repeated for all events.\n",
    "uncertainties = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "assert t_obs.size == uncertainties.size\n",
    "\n",
    "print(\"Observed data (arrival time in seconds w.r.t. the clock on the seismograph):\\n\", t_obs)\n",
    "print(\"Data shape:\\n\", t_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding prior information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have seen in the misfit definition, there are three parameters passed that refer to the prior. These are two parameters for a Gaussian prior on the velocity of the medium, as well as a hard limit for Earthquake source depth. In the actual function, you can see how these add to the misfit.\n",
    "\n",
    "The prior on the Gaussian is a quadratic term which is added to the misfit function, defined by parameters `v_mean` and `v_std`:\n",
    "\n",
    "$$\n",
    "\\chi(\\mathbf{m}) = \\chi_\\text{data}(\\mathbf{m}) + \\frac{(v_\\text{current_model} - v_\\text{mean})^2}{2 v_\\text{std}^2}\n",
    "$$\n",
    "\n",
    "Additionally, the depth of the Earthquake is limited to `depth_limit`, as well as being required to be below the surface of the 'Earth' (i.e. depth $> 0$). We can incorporate these strict boundaries by adding infinity to our misfit term when appropriate:\n",
    "\n",
    "$$\n",
    "\\chi_\\text{final}(\\mathbf{m}) = \\begin{cases}\n",
    "\\chi(\\mathbf{m}) \\quad 0 < \\text{depth} < \\text{depth_limit} \\\\\n",
    "\\infty \\quad else\\\\\n",
    "\\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.588250Z",
     "start_time": "2020-02-13T16:57:58.576573Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Define prior information on depth and velocity\n",
    "v_mean = 4.5\n",
    "v_std = 1\n",
    "depth_limit = 25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the inverse problem 'object' from the class \n",
    "\n",
    "Now we roll everything up into a simple class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.600890Z",
     "start_time": "2020-02-13T16:57:58.589711Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Collect the inverse problem into a object using the class 'template'\n",
    "target = target_tt(t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: An animated comparison of MH and HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prior to this notebook, I pre-computed samples by running HMC and MH for longer than 10 minutes. We'll use these samples as a reference distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:57:58.618471Z",
     "start_time": "2020-02-13T16:57:58.602111Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load a reference distribution. This is only for illustratory purposes.\n",
    "# You could e.g. also use the samples from a previous sampling attempt\n",
    "samples_MH_REF = np.load(\"ref_solutions/single_event_reference_MH_three_receivers.npy\")\n",
    "samples_HMC_REF = np.load(\"ref_solutions/single_event_reference_HMC_three_receivers.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T18:51:39.637045Z",
     "start_time": "2020-02-07T18:51:39.623897Z"
    }
   },
   "source": [
    "## Metropolis-Hastings sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Metropolis-Hastings algorithm is a very fundamental, useful and robust algorithm. The most common variant constructs a Markov chain over our target distribution the following way:\n",
    "\n",
    "1. Start at some initial model $m_\\text{start}$\n",
    "2. Draw a perturbation according to the proposal distribution $dm \\propto P(dm|m_\\text{start})$\n",
    "3. Propose a new model as $m_\\text{start} + dm = m_\\text{new}$\n",
    "4. Check if this new model is likely  (evaluate $P(m_\\text{new})$ ):\n",
    "    * If more likely, move there always.\n",
    "    * If less likely, move there $\\propto dP$\n",
    "5. Repeat by drawing new perturbation\n",
    "\n",
    "Investigating this thing visual is almost always a better idea. The file `samplers.py` provides a 'visual' MH sampler, which plots 2 dimensions of any distribution on the fly. Additionally, it allows us to plot a reference distribution in the background, e.g. a prior or a previous run.\n",
    "\n",
    "In the next notebook cell I load reference samples which I precomputed (from about 10 minutes of sampling) of exactly the same target distribution.\n",
    "\n",
    "**Try changing the step length `epsilon` and see what the algorithm does.** As a guideline, try to get the acceptance rate at 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:58:04.684451Z",
     "start_time": "2020-02-13T16:57:58.619539Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform the same sampling using MH\n",
    "\n",
    "m_start = np.array([16.1, 15.2, 14.9, 5.7])[:, None]\n",
    "\n",
    "# Tuning parameters =============================================================================================\n",
    "epsilon = 0.2\n",
    "number_of_samples = 50\n",
    "# End ===========================================================================================================\n",
    "\n",
    "# Select which dimensions to animate (2D at most is easiest on computer screens)\n",
    "dims_to_visualize = [2, 3]\n",
    "figsize=(7,7)\n",
    "animate_proposal=True # Disable this with 'False' if you find the animation annoying\n",
    "\n",
    "%matplotlib notebook\n",
    "samples_MH_1 = samplers.visual_sample_mh(\n",
    "    target,\n",
    "    m_start,\n",
    "    epsilon,\n",
    "    number_of_samples,\n",
    "    figsize=figsize,\n",
    "    dims_to_visualize=dims_to_visualize,\n",
    "    background_samples=samples_MH_REF,\n",
    "    true_m=m_true,\n",
    "    animate_sample_interval=1,\n",
    "    animate_proposal=animate_proposal, \n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see in generating a few samples using MH is that the distance traversed in an accepted move and acceptance rate trade-off; if I try to move greater distances, less samples are accepted. With only 100 samples, we do not approximate the distribution well, regardless of step length.\n",
    "\n",
    "Maybe there is an algorithm that an de better with such few samples ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 'physics': Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen in the presentation that HMC is harder to tune. We have a `mass matrix`, a step length (`dt`) and a number of steps (`nt`). \n",
    "\n",
    "**Try changing the tuning parameters to see what the effect is on the behaviour of the algorithm.** As a guideline, try to get the acceptance rate at 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:58:31.603832Z",
     "start_time": "2020-02-13T16:58:04.685557Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sampling using the basic HMC algorithm, but animated\n",
    "\n",
    "# Select a starting model\n",
    "m_start = np.array([16.1, 15.2, 14.9, 5.7])[:, None]\n",
    "\n",
    "# Choose which dimensions to animate\n",
    "dim_to_vis = [2, 3]\n",
    "figsize=(7,7)\n",
    "animate_trajectory=True # Disable this with 'False' if you find the animation annoying\n",
    "animate_trajectory_interval=10 # Lower this number to slow down the animation\n",
    "\n",
    "# Tuning parameters =============================================================================================\n",
    "# These next two are equivalent\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],  \n",
    "        [0, 1, 0, 0],  \n",
    "        [0, 0, 1, 0],  \n",
    "        [0, 0, 0, 1],\n",
    "    ], dtype=np.float32\n",
    ")  \n",
    "dt = 0.05\n",
    "nt = 100\n",
    "number_of_samples = 50\n",
    "\n",
    "# Sampling! =====================================================================================================\n",
    "%matplotlib notebook\n",
    "samples_HMC_1 = samplers.visual_sample_hmc(\n",
    "    target,\n",
    "    m_start,\n",
    "    nt,\n",
    "    dt,\n",
    "    number_of_samples,\n",
    "    mass_matrix,\n",
    "    figsize=figsize,\n",
    "    dims_to_visualize=dim_to_vis,\n",
    "    background_samples=samples_MH_REF,\n",
    "    true_m=m_true,\n",
    "    animate_trajectory=animate_trajectory,\n",
    "    animate_trajectory_interval=animate_trajectory_interval, # Lower this number to slow down the animation\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: A qualitative comparison of MH and HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an intuition of what the algorithms do, we can do a little more of a serious comparison. For this, we'll use un-animated algorithms, such that we get **maximum** speed. \n",
    "\n",
    "Additionally, we request 5000 proposals from each algorithm. That does not mean we get 5000 independent samples, just 5000 attempts to move to a new state. First we'll look at the perfomance (the time it takes to generated 5000 samples) and afterwards at the quality of these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:58:31.975449Z",
     "start_time": "2020-02-13T16:58:31.605206Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sampling using the basic MH algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:, None]\n",
    "\n",
    "# Tuning parameters\n",
    "epsilon = 0.15\n",
    "number_of_samples = 5000\n",
    "\n",
    "# Sampling\n",
    "%time samples_MH = samplers.sample_mh(target, m_start, epsilon, number_of_samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:58:56.899650Z",
     "start_time": "2020-02-13T16:58:31.976576Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sampling using the basic HMC algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:,np.newaxis]\n",
    "\n",
    "# Tuning parameters\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "    ], dtype=np.float32\n",
    ")  \n",
    "dt = 0.13\n",
    "nt = 40\n",
    "number_of_samples = 5000\n",
    "\n",
    "# Sampling\n",
    "%time samples_HMC = samplers.sample_hmc(target, m_start, nt, dt, number_of_samples, mass_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in performance for these algorithms is extreme. MH can generate 5000 samples in **under 0.5 seconds** on my machine (a 2018 high end laptop), while HMC takes **about 20 seconds**. \n",
    "\n",
    "But, performance isn't the only thing. The ability of a Markov chain to generate **independent** samples is just as important. To get a feeling of the *quality* of samples, we now compare the resulting two distributions to the reference distribution computed by **many** samples from HMC and MH.\n",
    "\n",
    "The next cell plots the marginals for both chains and compares them to the reference solutions. Additionally, **the true model values are plotted as the black vertical lines**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:58:58.757947Z",
     "start_time": "2020-02-13T16:58:56.900956Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize 1D marginals for all algorithms\n",
    "%matplotlib inline\n",
    "bins = int(5000 ** 0.4)\n",
    "\n",
    "limits = []\n",
    "\n",
    "figure, axes = plt.subplots(4, 3, figsize=(14, 14), constrained_layout=False)\n",
    "for i in range(4):\n",
    "    axes[i, 0].hist(samples_HMC[i, :], bins=bins, density=True)\n",
    "    ylim = axes[i, 0].get_ylim()\n",
    "    axes[i, 0].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "    axes[i, 0].set_ylim(ylim)\n",
    "    axes[i, 0].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "    axes[i, 0].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "    axes[i, 1].hist(samples_MH[i, :], bins=bins, density=True, color=\"k\")\n",
    "    ylim = axes[i, 1].get_ylim()\n",
    "    axes[i, 1].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "    axes[i, 1].set_ylim(ylim)\n",
    "    axes[i, 1].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "    axes[i, 1].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "    axes[i, 2].hist(\n",
    "        samples_HMC_REF[i, :], bins=bins, density=True, alpha=0.5, color=\"g\"\n",
    "    )\n",
    "    axes[i, 2].hist(samples_MH_REF[i, :], bins=bins, density=True, alpha=0.5, color=\"g\")\n",
    "    ylim = axes[i, 2].get_ylim()\n",
    "    axes[i, 2].plot([m_true[i], m_true[i]], [0, 1], \"k\")\n",
    "    axes[i, 2].set_ylim(ylim)\n",
    "    axes[i, 2].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "    axes[i, 2].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "    axes[i, 0].set_xlim(axes[i, 2].get_xlim())\n",
    "    axes[i, 1].set_xlim(axes[i, 2].get_xlim())\n",
    "\n",
    "    limits.append(axes[i, 2].get_xlim())\n",
    "\n",
    "\n",
    "axes[0, 0].set_title(f\"HMC {samples_HMC[0,:].size} samples\")\n",
    "axes[0, 1].set_title(f\"MH {samples_MH[0,:].size} samples\")\n",
    "axes[0, 2].set_title(\n",
    "    f\"Reference (MH+HMC) many samples\\n(resp. {samples_MH_REF[0,:].size:.2e} and {samples_HMC_REF[0,:].size:.2e})\"\n",
    ")\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in quality of the results is even more pronounced when looking at **higher order marginals**, which are visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:59:00.430029Z",
     "start_time": "2020-02-13T16:58:58.759166Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the Hamiltonian Monte Carlo 2D marginals\n",
    "marginal_grid(\n",
    "    samples_HMC,\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    "    color_1d=None,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:59:02.109840Z",
     "start_time": "2020-02-13T16:59:00.431664Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the Metropolis-Hastings 2D marginals\n",
    "\n",
    "marginal_grid(\n",
    "    samples_MH,\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:59:03.826023Z",
     "start_time": "2020-02-13T16:59:02.111139Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the reference solution 2D marginals\n",
    "marginal_grid(\n",
    "    samples_HMC_REF, # or samples_MH_REF\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    "    color_1d=\"green\",\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: What does it all mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've succesfully sampled the distributions with both MH and HMC. But let's now quickly go back to the actual inverse problem itself. This is note elementary when trying to understand the HMC algorithm, but it's maybe good for our physical intuition.\n",
    "\n",
    "Looking at the marginals of the previous notebook cells, we can see that there are **strong trade-offs between medium velocity, origin time, and vertical source location**. These parameters are quite unconstratined together. Horizontal source location seems to be much better resolved.\n",
    "\n",
    "When we visualize the marginal of hypocenter location (horizontal and vertical location, **in the next notebook cell**) we can see how poorly the inference constrains vertical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:59:04.001833Z",
     "start_time": "2020-02-13T16:59:03.827476Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of possible event locations\n",
    "axes = plt.axes()\n",
    "axes.hist2d(\n",
    "    samples_HMC_REF[0, :],\n",
    "    samples_HMC_REF[1, :],\n",
    "    bins=40,\n",
    "    range=[[-5, 35], [-3, depth_limit]],\n",
    "    cmap=plt.get_cmap(\"Greys\"),\n",
    ")\n",
    "axes.invert_yaxis()\n",
    "axes.set_xlabel(\"Horizontal location [km]\")\n",
    "axes.set_ylabel(\"Vertical location [km]\")\n",
    "\n",
    "axes.plot([-5, 35], [0, 0], \"k\")\n",
    "axes.scatter(m_true[0], m_true[1], 200, label=\"True location\")\n",
    "axes.scatter(station_x, station_z, 200, label=\"Seismographs\", marker=\"v\")\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:59:04.251123Z",
     "start_time": "2020-02-13T16:59:04.002919Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the knowledge on the medium velocity\n",
    "prior_velocities = np.random.randn(100000) * v_std + v_mean\n",
    "posterior_velocities = samples_HMC_REF[-2, :]\n",
    "\n",
    "plt.hist(\n",
    "    posterior_velocities, bins=50, density=True, label=\"Posterior velocity\"\n",
    ")\n",
    "plt.hist(\n",
    "    prior_velocities,\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    alpha=0.2,\n",
    "    color=\"k\",\n",
    "    label=\"Prior velocity\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Standard deviation prior to the experiment: {np.std(prior_velocities):.2f}\")\n",
    "print(f\"Standard deviation after to the experiment: {np.std(posterior_velocities):.2f}\")\n",
    "\n",
    "if np.std(posterior_velocities) < np.std(prior_velocities):\n",
    "    print(\"It seems like we learned something about the velocity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: The mass matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the influence of the mass matrix, we'll try to evaluate the exact same event, but with a much better knowledge of the medium velocity. Our prior knowledge on velocity is now centered at 4.9, with a much smaller 'spread' (standard deviation). A prior that is so informative is often called a **'strong prior'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T20:17:02.611639Z",
     "start_time": "2020-02-13T20:17:02.603846Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create an inference problem with a strong prior.\n",
    "\n",
    "v_mean = 4.95\n",
    "v_std = 0.04\n",
    "\n",
    "# Collect the inverse problem into a object using the class 'template'\n",
    "target_with_strong_prior = target_tt(t_obs, uncertainties, station_x, station_z, v_mean, v_std, depth_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have a target distribution with an updated (stronger) prior. \n",
    "\n",
    "We can now recreate the exact same HMC Markov chain (the same tuning settings) on this new distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:35:03.176028Z",
     "start_time": "2020-02-13T17:35:02.923319Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sample the strong prior target using the HMC algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:, np.newaxis]\n",
    "\n",
    "# Tuning parameters\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = np.array(\n",
    "    [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1],], dtype=np.float32\n",
    ")\n",
    "dt = 0.13\n",
    "nt = 40\n",
    "number_of_samples = 50\n",
    "\n",
    "# Sampling\n",
    "samples_HMC = samplers.sample_hmc_opt(\n",
    "    target_with_strong_prior, m_start, nt, dt, number_of_samples, mass_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh-oh! Something that worked before, doesn't work anymore. **Almost no accepted samples.** What could this be?\n",
    "\n",
    "Well, obviously, this is due to the change in target distribution. What likely happened, is that this target is much more constrained than the previous target (especially in the velocity dimension, number 4). The mass matrix was not updated accordingly, so sampling kind of failed (read: no samples are being generated).\n",
    "\n",
    "We can visualize this by animating the proposals. Again, we use a precomputed reference solution for visualization purposes, but this we wouldn't have a-priori.\n",
    "\n",
    "Once you understand what you might want to change in the tuning parameters, go ahead and **update the tuning parameters** s.t. you get an acceptance rate of about 50%. **What effect has your change on the trajectories**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T20:17:45.499658Z",
     "start_time": "2020-02-13T20:17:39.223776Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sample the strong prior problem, animated\n",
    "\n",
    "samples_HMC_strong_prior_REF = np.load(\n",
    "    \"ref_solutions/single_event_reference_HMC_three_receivers_strong_prior.npy\"\n",
    ")\n",
    "\n",
    "# Select a starting model\n",
    "m_start = np.array([16.1, 15.2, 14.9, 5.7])[:, None]\n",
    "\n",
    "# Choose which dimensions to animate\n",
    "dim_to_vis = [2, 3]\n",
    "figsize = (7, 7)\n",
    "animate_trajectory = (\n",
    "    True  # Disable this with 'False' if you find the animation annoying\n",
    ")\n",
    "animate_trajectory_interval = 10  # Lower this number to slow down the animation\n",
    "\n",
    "# Tuning parameters =============================================================================================\n",
    "mass_matrix = np.array(\n",
    "    [[1, 0, 0, 0], \n",
    "     [0, 1, 0, 0], \n",
    "     [0, 0, 1, 0], \n",
    "     [0, 0, 0, 10],], \n",
    "    dtype=np.float32\n",
    ")\n",
    "dt = 0.13\n",
    "nt = 40\n",
    "number_of_samples = 50\n",
    "\n",
    "# Sampling! =====================================================================================================\n",
    "%matplotlib notebook\n",
    "samples_HMC_strong_prior = samplers.visual_sample_hmc(\n",
    "    target_with_strong_prior,\n",
    "    m_start,\n",
    "    nt,\n",
    "    dt,\n",
    "    number_of_samples,\n",
    "    mass_matrix,\n",
    "    figsize=figsize,\n",
    "    dims_to_visualize=dim_to_vis,\n",
    "    background_samples=samples_HMC_strong_prior_REF,\n",
    "    true_m=m_true,\n",
    "    animate_trajectory=animate_trajectory,\n",
    "    animate_trajectory_interval=animate_trajectory_interval,  # Lower this number to slow down the animation\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the mass matrix helps a lot when the multidimensional distribution doesn't have a unit covariance matrix. This is of course almost never the case, which is why tuning the mass matrix is very important for HMC. There are many strategies, some use prior knowledge on the physics and some are automated. These techniques are beyond the scope of this notebook.\n",
    "\n",
    "Let's now quickly (using the reference solution take a look what effect updating the prior has on our solution. We sample the distribution with the updated mass matrix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T20:16:51.733777Z",
     "start_time": "2020-02-13T20:16:50.999839Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the result of the strong prior inference versus the original inference\n",
    "figure, axess = plt.subplots(1, 4, figsize=(14, 4))\n",
    "\n",
    "axess[0].set_ylabel(\"Relative likelihood\")\n",
    "\n",
    "for i in range(3):\n",
    "    axess[i].hist(\n",
    "        samples_HMC_strong_prior_REF[i, :],\n",
    "        bins=30,\n",
    "        label=\"Strong prior\",\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "    )\n",
    "    axess[i].hist(\n",
    "        samples_HMC_REF[i, :], bins=30, label=\"Original prior\", alpha=0.5, density=True\n",
    "    )\n",
    "    axess[i].set_xlabel(\"%s [%s]\" % (target.labels[i], target.units[i]))\n",
    "\n",
    "axess[-1].hist(\n",
    "    samples_HMC_strong_prior_REF[-2, :],\n",
    "    bins=30,\n",
    "    label=\"Strong prior\",\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    ")\n",
    "axess[-1].hist(\n",
    "    samples_HMC_REF[-2, 1000:],\n",
    "    bins=30,\n",
    "    label=\"Original prior\",\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    ")\n",
    "axess[-1].set_xlabel(\"%s [%s]\" % (target.labels[-2], target.units[-2]))\n",
    "\n",
    "axess[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the increased knowledge helped us delineat both origin time and vertical source location better. We can also see that the 2D marginals containign velocity almost 'collapse':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:35:49.801108Z",
     "start_time": "2020-02-13T17:35:48.211801Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the reference solution 2D marginals\n",
    "marginal_grid(\n",
    "    samples_HMC_strong_prior_REF,\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    "    color_1d=\"green\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: With as many samples, HMC typically gives better results compared to MH.\n",
    "\n",
    "But we've also seen that **HMC generates samples much slower**. In Notebook 2 we see a performance comparison when HMC and MH are pitted against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of the Notebook 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ‚Üì Bonus material is that way ‚Üì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Changing the acquisition set-up, changing the mass matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try our inversion once again, but now the data is observed by a **high-density** receiver network.\n",
    "\n",
    "Intuitively, if we use more stations, our solution should become less uncertain. What this would imply for the posterior is that **the spread (or more mathematically the covariance) would become smaller**.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:59:22.415536Z",
     "start_time": "2020-02-13T16:59:22.223335Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create a high-density receiver network\n",
    "\n",
    "# Define earthquake source properties\n",
    "source_x = 16.0\n",
    "source_z = 15.0\n",
    "origin_T = 17.0\n",
    "v_exact = 5.0\n",
    "\n",
    "# Define station coordinates of the array\n",
    "station_x_dense = np.linspace(0, 30, 15)\n",
    "station_z_dense = np.zeros_like(station_x_dense)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6,6))\n",
    "axes.scatter(station_x_dense, station_z_dense, color='b', marker='v', s=200, label='Receivers')\n",
    "axes.scatter(source_x, source_z, color='r', marker='.', s=400, label='True location')\n",
    "axes = plt.gca()\n",
    "axes.set_xlabel(\"Horizontal location [km]\")\n",
    "axes.set_ylabel(\"Vertical location [km]\")\n",
    "axes.plot([-10, 35], [0, 0], \"k\")\n",
    "axes.set_xlim([-10,35])\n",
    "axes.set_ylim([25,-2])\n",
    "axes.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:59:22.421072Z",
     "start_time": "2020-02-13T16:59:22.416571Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create the inverse problem from this high-density receiver network\n",
    "\n",
    "# Create a vector out of the true values\n",
    "m_true = np.array([source_x, source_z, origin_T, v_exact])\n",
    "\n",
    "# Calculate observed (exact) arrival times for all stations\n",
    "t_obs = forward(m_true, station_x_dense, station_z_dense)\n",
    "\n",
    "# Define uncertainties for the observed arrival time at each station. These are repeated for all events.\n",
    "uncertainties = np.ones_like(station_x_dense) * 0.2\n",
    "uncertainties[\n",
    "    0\n",
    "] = 0.5  # Just to recreate the first station from previous implementation\n",
    "\n",
    "assert t_obs.size == uncertainties.size\n",
    "\n",
    "# Prior information on depth and velocity\n",
    "v_mean = 4.5\n",
    "v_std = 1\n",
    "depth_limit = 25.0\n",
    "target_many_receivers = target_tt(\n",
    "    t_obs, uncertainties, station_x_dense, station_z_dense, v_mean, v_std, depth_limit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initially, sampling will not work. Try to find a mass matrix that does have a high acceptance rate for this target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:00:55.043170Z",
     "start_time": "2020-02-13T16:59:22.422271Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sample this inverse problem with exactly the same settings as the first targets\n",
    "\n",
    "# Sampling using the basic HMC algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:, np.newaxis]\n",
    "\n",
    "# Tuning parameters\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = 10 * np.array(\n",
    "    [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1],], dtype=np.float32\n",
    ")\n",
    "dt = 0.13\n",
    "nt = 40\n",
    "number_of_samples_dense = 5000\n",
    "\n",
    "# Sampling\n",
    "samples_HMC_many_receivers = samplers.sample_hmc_opt(\n",
    "    target_many_receivers, m_start, nt, dt, number_of_samples_dense, mass_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:00:56.643235Z",
     "start_time": "2020-02-13T17:00:55.044546Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the reference solution 2D marginals\n",
    "marginal_grid(\n",
    "    samples_HMC_many_receivers,\n",
    "    [0, 1, 2, 3],\n",
    "    bins=30,\n",
    "    labels=[f\"{l}\\n[{u}]\" for l, u in zip(target.labels, target.units)],\n",
    "    color_1d=\"green\",\n",
    "    limits=limits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evidently, the updated receiver network strongly improves the quality of locating Earthquakes, but doesn't give us much more information on the medium parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMC reference samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:00:56.649336Z",
     "start_time": "2020-02-13T17:00:56.644790Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Reference solution\n",
    "\n",
    "m_start = np.array([16.1, 15.2, 14.9, 5.7])[:, None]\n",
    "\n",
    "epsilon = 0.15\n",
    "nt = 10\n",
    "number_of_samples_ref = 4000000\n",
    "\n",
    "mass_matrix = np.eye(4)\n",
    "\n",
    "# Uncomment to run\n",
    "# samples_HMC_REF = samplers.sample_hmc_opt(\n",
    "#     target, m_start, nt, epsilon, number_of_samples_ref, mass_matrix, thinning=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:00:56.664878Z",
     "start_time": "2020-02-13T17:00:56.650941Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(\"ref_solutions/single_event_reference_HMC_three_receivers.npy\", samples_HMC_REF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:09:20.701210Z",
     "start_time": "2020-02-13T16:09:20.696092Z"
    }
   },
   "source": [
    "### Metropolis-Hastings reference samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:00:56.677361Z",
     "start_time": "2020-02-13T17:00:56.666108Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sample using the basic MH algorithm\n",
    "m_start = np.array([16.1, 15.2, 14.9, 5.7])[:, None]\n",
    "\n",
    "epsilon = 0.15\n",
    "number_of_samples = 4000000\n",
    "\n",
    "# Uncomment to run\n",
    "# samples_MH_REF = samplers.sample_mh(target, m_start, epsilon, number_of_samples, thinning=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T17:00:56.691928Z",
     "start_time": "2020-02-13T17:00:56.678738Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(\"ref_solutions/single_event_reference_MH_three_receivers.npy\", samples_MH_REF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMC reference samples for the strong prior problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:36:48.940238Z",
     "start_time": "2020-02-13T18:12:05.966118Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sample using the basic HMC algorithm\n",
    "m_start = np.array([16.1, 15.2, 17.3, 4.7])[:, np.newaxis]\n",
    "\n",
    "# Tuning parameters\n",
    "# mass_matrix = np.eye(4)\n",
    "mass_matrix = np.array(\n",
    "    [\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 10],\n",
    "    ], dtype=np.float32\n",
    ")  \n",
    "dt = 0.13\n",
    "nt = 40\n",
    "number_of_samples = 360000\n",
    "\n",
    "# Sampling\n",
    "samples_HMC_strong_prior_REF = samplers.sample_hmc_opt(\n",
    "    target_with_strong_prior, m_start, nt, dt, number_of_samples, mass_matrix, thinning=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:36:49.186852Z",
     "start_time": "2020-02-13T18:36:49.183681Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"ref_solutions/single_event_reference_HMC_three_receivers_strong_prior.npy\", samples_HMC_strong_prior_REF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the max and min of the original distribution, such that plotting limits are forced to be equal between the two targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T18:36:49.064809Z",
     "start_time": "2020-02-13T18:36:49.060795Z"
    }
   },
   "outputs": [],
   "source": [
    "mins = np.min(samples_HMC_REF, axis=1)[:,None]\n",
    "maxs = np.max(samples_HMC_REF, axis=1)[:,None]\n",
    "\n",
    "samples_HMC_strong_prior_REF = np.hstack((samples_HMC_strong_prior_REF,maxs,mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "981px",
    "left": "1648px",
    "right": "20px",
    "top": "144px",
    "width": "456px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
